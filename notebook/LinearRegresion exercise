In [1]:

print "LinearRegresion Exercise"

print "Q1"

 

import numpy as np

 

data = np.array([map(float, l.split()) for l in open('housing.data')])

# agafem l' úĺtima columna

Y = data[:,-1]

print "number of instances", len (Y)

print Y.T

LinearRegresion Exercise
Q1
number of instances 506
[ 24.   21.6  34.7  33.4  36.2  28.7  22.9  27.1  16.5  18.9  15.   18.9
  21.7  20.4  18.2  19.9  23.1  17.5  20.2  18.2  13.6  19.6  15.2  14.5
  15.6  13.9  16.6  14.8  18.4  21.   12.7  14.5  13.2  13.1  13.5  18.9
  20.   21.   24.7  30.8  34.9  26.6  25.3  24.7  21.2  19.3  20.   16.6
  14.4  19.4  19.7  20.5  25.   23.4  18.9  35.4  24.7  31.6  23.3  19.6
  18.7  16.   22.2  25.   33.   23.5  19.4  22.   17.4  20.9  24.2  21.7
  22.8  23.4  24.1  21.4  20.   20.8  21.2  20.3  28.   23.9  24.8  22.9
  23.9  26.6  22.5  22.2  23.6  28.7  22.6  22.   22.9  25.   20.6  28.4
  21.4  38.7  43.8  33.2  27.5  26.5  18.6  19.3  20.1  19.5  19.5  20.4
  19.8  19.4  21.7  22.8  18.8  18.7  18.5  18.3  21.2  19.2  20.4  19.3
  22.   20.3  20.5  17.3  18.8  21.4  15.7  16.2  18.   14.3  19.2  19.6
  23.   18.4  15.6  18.1  17.4  17.1  13.3  17.8  14.   14.4  13.4  15.6
  11.8  13.8  15.6  14.6  17.8  15.4  21.5  19.6  15.3  19.4  17.   15.6
  13.1  41.3  24.3  23.3  27.   50.   50.   50.   22.7  25.   50.   23.8
  23.8  22.3  17.4  19.1  23.1  23.6  22.6  29.4  23.2  24.6  29.9  37.2
  39.8  36.2  37.9  32.5  26.4  29.6  50.   32.   29.8  34.9  37.   30.5
  36.4  31.1  29.1  50.   33.3  30.3  34.6  34.9  32.9  24.1  42.3  48.5
  50.   22.6  24.4  22.5  24.4  20.   21.7  19.3  22.4  28.1  23.7  25.
  23.3  28.7  21.5  23.   26.7  21.7  27.5  30.1  44.8  50.   37.6  31.6
  46.7  31.5  24.3  31.7  41.7  48.3  29.   24.   25.1  31.5  23.7  23.3
  22.   20.1  22.2  23.7  17.6  18.5  24.3  20.5  24.5  26.2  24.4  24.8
  29.6  42.8  21.9  20.9  44.   50.   36.   30.1  33.8  43.1  48.8  31.
  36.5  22.8  30.7  50.   43.5  20.7  21.1  25.2  24.4  35.2  32.4  32.
  33.2  33.1  29.1  35.1  45.4  35.4  46.   50.   32.2  22.   20.1  23.2
  22.3  24.8  28.5  37.3  27.9  23.9  21.7  28.6  27.1  20.3  22.5  29.
  24.8  22.   26.4  33.1  36.1  28.4  33.4  28.2  22.8  20.3  16.1  22.1
  19.4  21.6  23.8  16.2  17.8  19.8  23.1  21.   23.8  23.1  20.4  18.5
  25.   24.6  23.   22.2  19.3  22.6  19.8  17.1  19.4  22.2  20.7  21.1
  19.5  18.5  20.6  19.   18.7  32.7  16.5  23.9  31.2  17.5  17.2  23.1
  24.5  26.6  22.9  24.1  18.6  30.1  18.2  20.6  17.8  21.7  22.7  22.6
  25.   19.9  20.8  16.8  21.9  27.5  21.9  23.1  50.   50.   50.   50.
  50.   13.8  13.8  15.   13.9  13.3  13.1  10.2  10.4  10.9  11.3  12.3
   8.8   7.2  10.5   7.4  10.2  11.5  15.1  23.2   9.7  13.8  12.7  13.1
  12.5   8.5   5.    6.3   5.6   7.2  12.1   8.3   8.5   5.   11.9  27.9
  17.2  27.5  15.   17.2  17.9  16.3   7.    7.2   7.5  10.4   8.8   8.4
  16.7  14.2  20.8  13.4  11.7   8.3  10.2  10.9  11.    9.5  14.5  14.1
  16.1  14.3  11.7  13.4   9.6   8.7   8.4  12.8  10.5  17.1  18.4  15.4
  10.8  11.8  14.9  12.6  14.1  13.   13.4  15.2  16.1  17.8  14.9  14.1
  12.7  13.5  14.9  20.   16.4  17.7  19.5  20.2  21.4  19.9  19.   19.1
  19.1  20.1  19.9  19.6  23.2  29.8  13.8  13.3  16.7  12.   14.6  21.4
  23.   23.7  25.   21.8  20.6  21.2  19.1  20.6  15.2   7.    8.1  13.6
  20.1  21.8  24.5  23.1  19.7  18.3  21.2  17.5  16.8  22.4  20.6  23.9
  22.   11.9]

In [2]:

print "LinearRegresion Exercise"

print "Q2"

 

Y_mean = np.mean(Y)

print "mean is %d" %(Y_mean)

LinearRegresion Exercise
Q2
mean is 22

print "LinearRegresion Exercise" print "Q2" Y_mean=np.mean(Y) print "average:", average
In [5]:

mse = sum ((Y_mean-Y)**2) / len(Y)

print "mean squared error:", mse

mean squared error: 84.4195561562

In [6]:

def MSE(y_p, y):

 

    return np.sum((y-y_p)**2) / len(y)

 

print MSE(Y_mean, Y)

84.4195561562

In [8]:

print "LinearRegresion Exercise"

print "Q3"

 

n = len(Y)/2

 

    

traindata = data[:n,:-1]

trainY = Y[:n]

testdata = data[n:,:-1]

testY = Y[:n]

    

MSEtrain = []

MSEtest = []

theta = []

 

 

for i in range(0, len(traindata[0])):

 

    train = np.hstack((np.ones((n,1)), traindata[:, i:i+1]))    

    test = np.hstack((np.ones((n,1)), testdata[:, i:i+1]))

    theta = np.linalg.lstsq(train, trainY)[0]

    MSEtrain.append(MSE(np.dot(train, theta), trainY))

    MSEtest.append(MSE(np.dot(test, theta), testY))

 

    R = 1 - MSEtest[i]/np.var(testY)

 

    print "Columna %d, R=%3.f" % (i,R)

LinearRegresion Exercise
Q3
Columna 0, R=-17
Columna 1, R= -0
Columna 2, R= -0
Columna 3, R= -0
Columna 4, R= -0
Columna 5, R= -1
Columna 6, R= -0
Columna 7, R= -0
Columna 8, R= -0
Columna 9, R= -1
Columna 10, R= -0
Columna 11, R= -1
Columna 12, R= -1

In [20]:

minMSE = []

 

for i in range(len(MSEtrain)):

    minMSE.append(abs(MSEtrain[i] - MSEtest[i]))

print "Worst WSE at column %d with value %3.f" % (np.argmax(MSEtest), np.max(MSEtest))

 

Worst WSE at column 0 with value 1236

In [29]:

print "LinearRegresion Exercise"

print "Q4"

train2=traindata[:, 1:]

test2=testdata[:,1:]

 

MSEtrain = []

MSEtest = []

theta = []

 

for i in range(0, len(train2[0])):

        train = np.hstack((np.ones((n,1)), train2[:, i:i+1]))

        test  = np.hstack((np.ones((n,1)), test2[:, i:i+1]))

 

        theta = np.linalg.lstsq(train, trainY)[0]

 

        MSEtrain.append(MSE(np.dot(train, theta), trainY))

        MSEtest.append(MSE(np.dot(test, theta), testY))

 

        R = 1 - MSEtest[i]/np.var(testY)

 

        print "Columna %d, R=%3.f" % (i,R)

 

LinearRegresion Exercise
Q4
Columna 0, R= -0
Columna 1, R= -0
Columna 2, R= -0
Columna 3, R= -0
Columna 4, R= -1
Columna 5, R= -0
Columna 6, R= -0
Columna 7, R= -0
Columna 8, R= -1
Columna 9, R= -0
Columna 10, R= -1
Columna 11, R= -1

In [30]:

minMSE = []

 

for i in range(len(MSEtrain)):

    minMSE.append(abs(MSEtrain[i] - MSEtest[i]))

 

print "Now, worst WSE at column %d with value %3.f" % (np.argmax(MSEtest), np.max(MSEtest))

 

Now, worst WSE at column 11 with value 142

In [38]:

print "LinearRegresion Exercise"

print "Q4"

MSEtrain = []

MSEtest = []

theta = []

 

for i in range(0, len(traindata[0])):

        train = np.hstack((np.ones((n,1)), traindata[:, i:i+1]))

        train = np.hstack((train, traindata[:, i:i+1]**2))

        train = np.hstack((train, traindata[:, i:i+1]**3))

        train = np.hstack((train, traindata[:, i:i+1]**4))

 

        test  = np.hstack((np.ones((n,1)), testdata[:, i:i+1]))

        test = np.hstack((test, testdata[:, i:i+1]**2))

        test = np.hstack((test, testdata[:, i:i+1]**3))

        test = np.hstack((test, testdata[:, i:i+1]**4))

 

        theta = np.linalg.lstsq(train, trainY)[0]

 

        MSEtrain.append(MSE(np.dot(train, theta), trainY))

        MSEtest.append(MSE(np.dot(test, theta), testY))

 

        R = 1 - MSEtest[i]/np.var(testY)

 

        print "Columna %d, R=%3.f" % (i,R)

 

minMSE = []

 

for i in range(len(MSEtrain)):

    minMSE.append(abs(MSEtrain[i] - MSEtest[i]))

 

print "Worst WSE at column %d with value %3.f" % (np.argmax(MSEtest), np.max(MSEtest))

LinearRegresion Exercise
Q4
Columna 0, R=-723020788060
Columna 1, R= -0
Columna 2, R= -0
Columna 3, R= -0
Columna 4, R= -0
Columna 5, R= -1
Columna 6, R= -0
Columna 7, R= -0
Columna 8, R=-10574
Columna 9, R=-3394
Columna 10, R= -0
Columna 11, R= -2
Columna 12, R= -1
Worst WSE at column 0 with value 50071549339421

In [90]:

 


